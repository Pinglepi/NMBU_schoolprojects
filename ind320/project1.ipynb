{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "August Noer Steinset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a local database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up cassandra and spark, most paths are registered with the OS enviromental varaibles, but the python ones had to be included in the jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\"\n",
    "\n",
    "# Connecting to Cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating some data with cassandra and showing it with spark. This is to show that it works, nothing will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\augus\\Documents\\Skole\\git\\NMBU_schoolprojects\\ind320\\project1.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO project1.table1 (id, name, age, city, job) VALUES (6, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mAnna\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, 28, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mLondon\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mTeacher\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Creating a dataframe\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.sql.cassandra\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(table\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtable1\u001b[39;49m\u001b[39m\"\u001b[39;49m, keyspace\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproject1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Creating a view\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload())\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Deleting the keyspace and the table\n",
    "session.execute(\"DROP TABLE IF EXISTS project1.table1\")\n",
    "session.execute(\"DROP TABLE IF EXISTS project1.table2\")\n",
    "session.execute(\"DROP TABLE IF EXISTS project1.table3\")\n",
    "session.execute(\"DROP KEYSPACE IF EXISTS project1\")\n",
    "\n",
    "# Creating a keyspace\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS project1 WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1}\")\n",
    "session.set_keyspace('project1')\n",
    "\n",
    "# Creating a table\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS project1.table1 (id int PRIMARY KEY, name text, age int, city text, job text)\")\n",
    "\n",
    "# Inserting data into the table\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (1, 'John', 25, 'London', 'Engineer')\")\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (2, 'Anna', 22, 'Paris', 'Teacher')\")\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (3, 'Peter', 29, 'Berlin', 'Doctor')\")\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (4, 'Linda', 33, 'New York', 'Lawyer')\")\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (5, 'John', 42, 'Paris', 'Engineer')\")\n",
    "session.execute(\"INSERT INTO project1.table1 (id, name, age, city, job) VALUES (6, 'Anna', 28, 'London', 'Teacher')\")\n",
    "\n",
    "# Creating a dataframe\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table1\", keyspace=\"project1\").load()\n",
    "df.show()\n",
    "\n",
    "# Creating a view\n",
    "df.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "# Querying the view\n",
    "df2 = spark.sql(\"SELECT * FROM table1 WHERE age > 30\")\n",
    "df2.show()\n",
    "\n",
    "# Deleting the keyspace and the table\n",
    "session.execute(\"DROP TABLE IF EXISTS project1.table1\")\n",
    "session.execute(\"DROP KEYSPACE IF EXISTS project1\")\n",
    "\n",
    "# See if the table is still there\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table1\", keyspace=\"project1\").load()\n",
    "try:\n",
    "    df.show()\n",
    "except:\n",
    "    print(\"The table is not there anymore.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some groundwork to be able to donwload the data, code mostly stolen from https://github.com/barentswatch/barentswatch-api-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from file secrets.txt and store the client id and client secret\n",
    "with open('secrets.txt', 'r') as f:\n",
    "\tclient_id = f.readline().strip()\n",
    "\tclient_secret = f.readline().strip()\n",
    "\n",
    "config = {\n",
    "\t'client_id': client_id,\n",
    "\t'client_secret': client_secret,\n",
    "\t'token_url': 'https://id.barentswatch.no/connect/token',\n",
    "\t'api_base_url': 'https://www.barentswatch.no/bwapi'\n",
    "}\n",
    "\n",
    "def get_token():\n",
    "  if not config['client_id']:\n",
    "    raise ValueError('client_id must be set in credentials.py')\n",
    "\n",
    "  if not config['client_secret']:\n",
    "    raise ValueError('client_secret must be set in credentials.py')\n",
    "\n",
    "  req = requests.post(config['token_url'],\n",
    "    data={\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': config['client_id'],\n",
    "        'client_secret': config['client_secret'],\n",
    "        'scope': 'api'\n",
    "    },\n",
    "    headers={'content-type': 'application/x-www-form-urlencoded'})\n",
    "\n",
    "  req.raise_for_status()\n",
    "  print('Token request successful')\n",
    "  return req.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first API requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token request successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_week_summary(token, year, week):\n",
    "  url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{year}/{week}\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()\n",
    "\n",
    "def get_localities(token):\n",
    "  url = f\"{config['api_base_url']}/v1/geodata/fishhealth/localities\"\n",
    "  headers ={\n",
    "    'authorization': 'Bearer ' + token['access_token'],\n",
    "    'content-type': 'application/json',\n",
    "  }\n",
    "  response = requests.get(url, headers=headers)\n",
    "  response.raise_for_status()\n",
    "  return response.json()\n",
    "\n",
    "def get_year_summary(token, year):\n",
    "    year_list = []\n",
    "    for i in range(52):\n",
    "        weeksummary = get_week_summary(token, year, str(i+1))\n",
    "        year_list.append(weeksummary)\n",
    "    return year_list\n",
    "\n",
    "token = get_token()\n",
    "yearsummary = get_year_summary(token, '2020')\n",
    "locality = get_localities(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'localityNo': 35297, 'localityWeekId': 1033452, 'name': 'Ådnøy Sø', 'hasReportedLice': True, 'isFallow': False, 'avgAdultFemaleLice': 0.01, 'hasCleanerfishDeployed': False, 'hasMechanicalRemoval': False, 'hasSubstanceTreatments': False, 'hasPd': False, 'hasIla': False, 'municipalityNo': '1108', 'municipality': 'Sandnes', 'lat': 58.91455, 'lon': 6.028583, 'isOnLand': False, 'inFilteredSelection': True, 'hasSalmonoids': True, 'isSlaughterHoldingCage': False}\n"
     ]
    }
   ],
   "source": [
    "# find a locality with lice for further use\n",
    "\n",
    "event = 0\n",
    "for i in yearsummary[0]['localities']:\n",
    "    if event != 0:\n",
    "        break\n",
    "    if i['hasReportedLice']:\n",
    "        event = i\n",
    "    \n",
    "print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.028583\n",
      "58.91455\n"
     ]
    }
   ],
   "source": [
    "# find a locality in yearsummary where the localityNo is the same as the localityNo in event\n",
    "yearsummary_locality = 0\n",
    "for i in yearsummary:\n",
    "    if yearsummary_locality != 0:\n",
    "        break\n",
    "    for j in i['localities']:\n",
    "        if j['localityNo'] == event['localityNo']:\n",
    "            yearsummary_locality = j\n",
    "\n",
    "# get the longitude and latitude of the locality\n",
    "longitude = yearsummary_locality['lon']\n",
    "latitude = yearsummary_locality['lat']\n",
    "\n",
    "#print the longitude and latitude\n",
    "print(longitude)\n",
    "print(latitude)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token request successful\n"
     ]
    }
   ],
   "source": [
    "# Select one locality and download lice count data (localityWeek) for the same year from: /v1/geodata/fishhealth/locality/{localityNo}/{week}\n",
    "\n",
    "def get_locality_week(token, localityNo, year, week):\n",
    "    url = f\"{config['api_base_url']}/v1/geodata/fishhealth/locality/{localityNo}/{year}/{week}\"\n",
    "    headers ={\n",
    "        'authorization': 'Bearer ' + token['access_token'],\n",
    "        'content-type': 'application/json',\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_year_locality(token, localityNo, year):\n",
    "    year_list = []\n",
    "    for i in range(52):\n",
    "        weeksummary = get_locality_week(token, localityNo, year, str(i+1))\n",
    "        year_list.append(weeksummary)\n",
    "    return year_list\n",
    "\n",
    "\n",
    "token = get_token()\n",
    "localityyear = get_year_locality(token, event['localityNo'], '2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the database with cassandra. This includes three creativly named tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x26503eb22d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS project1 WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1}\")\n",
    "session.set_keyspace('project1')\n",
    "\n",
    "# Creating a table\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS project1.table1 (aquaCultureRegistryVersion int, localityNo int PRIMARY KEY, name text, municipalityNo int, municipality text)\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS project1.table2 (year int, week int PRIMARY KEY, localities text)\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS project1.table3 (year int,\tweek int PRIMARY KEY, hasReportedLice boolean, avgAdultFemaleLice float, avgMobileLice float, avgStationaryLice float)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First all the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Attempting to write to C* Table but missing\nprimary key columns: [id]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\augus\\Documents\\Skole\\git\\NMBU_schoolprojects\\ind320\\project1.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m locality_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(locality)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m locality_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m locality_df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m spark\u001b[39m.\u001b[39;49mcreateDataFrame(locality_df)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.sql.cassandra\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m.\u001b[39;49moptions(table\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtable1\u001b[39;49m\u001b[39m\"\u001b[39;49m, keyspace\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproject1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.cassandra\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39moptions(table\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtable1\u001b[39m\u001b[39m\"\u001b[39m, keyspace\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mproject1\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mload()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/augus/Documents/Skole/git/NMBU_schoolprojects/ind320/project1.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mformat\u001b[39m)\n\u001b[0;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1461\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave(path)\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\augus\\miniconda3\\envs\\IND320\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Attempting to write to C* Table but missing\nprimary key columns: [id]"
     ]
    }
   ],
   "source": [
    "# convert locality(list of dictionaries) to dataframe\n",
    "locality_df = pd.DataFrame(locality)\n",
    "locality_df.columns = locality_df.columns.str.lower()\n",
    "\n",
    "spark.createDataFrame(locality_df).write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"table1\", keyspace=\"project1\").mode(\"append\").save()\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table1\", keyspace=\"project1\").load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then data for an entire year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = pd.DataFrame(yearsummary)\n",
    "df_year.columns = df_year.columns.str.lower()\n",
    "\n",
    "#convert list of dictionaries to json\n",
    "df_year['localities'] = df_year['localities'].apply(json.dumps)\n",
    "\n",
    "spark.createDataFrame(df_year).write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"table2\", keyspace=\"project1\").mode(\"append\").save()\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table2\", keyspace=\"project1\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last the data for a single location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert localityyear(list of dictionaries) to dataframe\n",
    "df = pd.DataFrame(localityyear)\n",
    "df_localityyear = pd.json_normalize(df['liceCountPreviousWeek'])\n",
    "df_localityyear.columns = df_localityyear.columns.str.lower()\n",
    "\n",
    "df_localityyear.head()\n",
    "\n",
    "df_localityyear = df_localityyear.astype({'avgadultfemalelice': 'float64', 'avgmobilelice': 'float64', 'avgstationarylice': 'float64', 'hasreportedlice': 'bool', 'week': 'int64', 'year': 'int64'})\n",
    "\n",
    "spark.createDataFrame(df_localityyear).write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"table3\", keyspace=\"project1\").mode(\"append\").save()\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table3\", keyspace=\"project1\").load()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the percentage of locations that had reported PB each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_sdf = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table2\", keyspace=\"project1\").load().toPandas()\n",
    "yearly_sdf['localities'] = yearly_sdf['localities'].apply(json.loads)\n",
    "\n",
    "yearly_sdf = yearly_sdf.sort_values(by=['week'])\n",
    "yearly_sdf = yearly_sdf.set_index('week')\n",
    "\n",
    "\n",
    "list_pb = []\n",
    "for i in yearly_sdf['localities']:\n",
    "    count_pb = 0\n",
    "    for j in i:\n",
    "        if j['hasPd']:\n",
    "            count_pb += 1\n",
    "    list_pb.append(count_pb/len(i))\n",
    "    count_pb = 0\n",
    "\n",
    "# plot list_pb\n",
    "x = np.arange(1,53)\n",
    "y = list_pb\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Proportion of localities that have reported PD')\n",
    "plt.title('Proportion of localities that have reported PD each week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average amount of adult female lice for one location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lice_data = spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table3\", keyspace=\"project1\").load().toPandas()\n",
    "\n",
    "# sort the dataframe by week and set week as index\n",
    "lice_data = lice_data.sort_values(by=['week'])\n",
    "lice_data = lice_data.set_index('week')\n",
    "\n",
    "# plot avgadultfemalelice against week\n",
    "lice_data.plot(y='avgadultfemalelice')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('avgadultfemalelice')\n",
    "plt.title('avgadultfemalelice against week')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And adding in the other types of lice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot proportions of different lice types\n",
    "lice_data.plot(y=['avgadultfemalelice', 'avgmobilelice', 'avgstationarylice'])\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Proportion')\n",
    "plt.title('Proportions of different lice types')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, delete the data and shut down the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the keyspace and the table\n",
    "#session.execute(\"DROP TABLE IF EXISTS project1.table1\")\n",
    "#session.execute(\"DROP TABLE IF EXISTS project1.table2\")\n",
    "#session.execute(\"DROP TABLE IF EXISTS project1.table3\")\n",
    "#session.execute(\"DROP KEYSPACE IF EXISTS project1\")\n",
    "\n",
    "# Closing the connection\n",
    "session.shutdown()\n",
    "cluster.shutdown()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
